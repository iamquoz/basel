{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import miniaudio\n",
    "\n",
    "def choose_device():\n",
    "    devices = miniaudio.Devices()\n",
    "    print(\"Available recording devices:\")\n",
    "    captures = devices.get_captures()\n",
    "    for d in enumerate(captures):\n",
    "        print(\"{num} = {name}\".format(num=d[0], name=d[1]['name']))\n",
    "    choice = int(input(\"record from which device? \"))\n",
    "    return captures[choice]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_chunks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available recording devices:\n",
      "0 = Microphone (USB Live Camera audio)\n",
      "1 = Microphone (fifine Microphone)\n",
      "..."
     ]
    }
   ],
   "source": [
    "import array\n",
    "\n",
    "\n",
    "def record_to_buffer():\n",
    "    _ = yield\n",
    "    while True:\n",
    "        data = yield\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        buffer_chunks.append(data)\n",
    "\n",
    "capture = miniaudio.CaptureDevice(buffersize_msec=1000, sample_rate=44100, device_id=choose_device()['id'])\n",
    "generator = record_to_buffer()\n",
    "next(generator)\n",
    "capture.start(generator)\n",
    "input('Enter to stop recording')\n",
    "capture.stop()\n",
    "buffer = b\"\".join(buffer_chunks)\n",
    "samples = array.array('h')\n",
    "samples.frombytes(buffer)\n",
    "sound = miniaudio.DecodedSoundFile('capture', capture.nchannels, capture.sample_rate, capture.format, samples)\n",
    "miniaudio.wav_write_file('capture.wav', sound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisperx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "torch.backends.cudnn.allow_tf32 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint c:\\Users\\andrew\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisperx\\assets\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No language specified, language will be first be detected for each audio file (increases inference time).\n",
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.5.1+cu121. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" \n",
    "audio_file = \"capture.wav\"\n",
    "batch_size = 16 # reduce if low on GPU mem\n",
    "compute_type = \"float16\" # change to \"int8\" if low on GPU mem (may reduce accuracy)\n",
    "\n",
    "# save model to local path (optional)\n",
    "model_dir = \"model\"\n",
    "model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type, download_root=model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: audio is shorter than 30s, language detection may be inaccurate.\n",
      "Detected language: en (0.64) in first 30s of audio...\n",
      "[{'text': ' Who am I talking to?', 'start': 1.178, 'end': 2.242}]\n"
     ]
    }
   ],
   "source": [
    "audio = whisperx.load_audio(audio_file)\n",
    "result = model.transcribe(audio, batch_size=batch_size)\n",
    "print(result[\"segments\"]) # before alignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Who am I talking to?']\n"
     ]
    }
   ],
   "source": [
    "segments = [segment[\"text\"] for segment in result[\"segments\"]]\n",
    "print(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pymorphy3.opencorpora_dict.wrapper:Loading dictionaries from c:\\Users\\andrew\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pymorphy3_dicts_ru\\data\n",
      "INFO:pymorphy3.opencorpora_dict.wrapper:format: 2.4, revision: 417150, updated: 2022-01-08T22:09:24.565962\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['who am i talking to'], 1)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pymorphy3\n",
    "\n",
    "morph = pymorphy3.MorphAnalyzer()\n",
    "nltk_stopwords = nltk.corpus.stopwords.words('russian')\n",
    "\n",
    "corpuses = []\n",
    "\n",
    "for text in segments:\n",
    "\ttext = text.lower()\n",
    "\twords = nltk.word_tokenize(text, language='russian')\n",
    "\twords = [word for word in words if word not in nltk_stopwords]\n",
    "\twords = [morph.parse(word)[0].normal_form for word in words]\n",
    "\twords = [word for word in words if word.isalpha()]\n",
    "\t\n",
    "\tcorpuses.append(' '.join(words))\n",
    "\n",
    "\n",
    "corpuses, len(corpuses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "MISTRAL_URI = 'http://localhost:11434'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt = \"You are a voice-powered neural network, every prompt you receive might be a question, a command, etc\" \\\n",
    "\"Please respond with a single word or a short phrase. If you don't know the answer, just say 'I don't know'\" \\\n",
    "\"Anything you'll recieve as a prompt will be stripped of filler words, so you can focus on the main idea\" \\\n",
    "\"You may recieve prompts in either english or russian\" \\\n",
    "\"Respond in the same language you were asked\" \\\n",
    "\"Please, don't try to be funny or sarcastic, just be helpful and informative\" \\\n",
    "\"Remember, you are a voice-powered neural network, you are here to help and inform, not to entertain\" \\\n",
    "\"Please, be polite and respectful, and always try to provide the most accurate and relevant information\" \\\n",
    "\"Thank you for your cooperation, and remember, you are a voice-powered neural network, you are here to help and inform, not to entertain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:11434/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Assistant\n"
     ]
    }
   ],
   "source": [
    "from mistralai import Mistral\n",
    "\n",
    "client = Mistral(server_url=MISTRAL_URI)\n",
    "\n",
    "for corpus in corpuses:\n",
    "\tresp = client.chat.complete(\n",
    "\t\tmodel='mistral',\n",
    "\t\tmessages=[\n",
    "\t\t\t{ 'role': 'system', 'content': sys_prompt },\n",
    "\t\t\t{ 'role': 'user', 'content': corpus }\n",
    "\t\t],\n",
    "\t\tmax_tokens=128\n",
    "\t)\n",
    "\n",
    "\tprint(resp.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
